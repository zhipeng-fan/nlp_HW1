{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "import pdb\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS_PATH = \"./aclImdb/train/pos/\"\n",
    "TRAIN_NEG_PATH = \"./aclImdb/train/neg/\"\n",
    "TEST_POS_PATH = \"./aclImdb/test/pos/\"\n",
    "TEST_NEG_PATH = \"./aclImdb/test/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(20000/2)\n",
    "\n",
    "train_pos_paths = glob(os.path.join(TRAIN_POS_PATH, \"*.txt\"))\n",
    "train_neg_paths = glob(os.path.join(TRAIN_NEG_PATH, \"*.txt\"))\n",
    "test_pos_paths = glob(os.path.join(TEST_POS_PATH, \"*.txt\"))\n",
    "test_neg_paths = glob(os.path.join(TEST_NEG_PATH, \"*.txt\"))\n",
    "\n",
    "random.Random(SEED).shuffle(train_pos_paths)\n",
    "random.Random(SEED).shuffle(train_neg_paths)\n",
    "\n",
    "val_pos_paths = train_pos_paths[train_split:]\n",
    "val_neg_paths = train_neg_paths[train_split:]\n",
    "\n",
    "train_pos_paths = train_pos_paths[:train_split]\n",
    "train_neg_paths = train_neg_paths[:train_split]\n",
    "\n",
    "train_paths = train_pos_paths+train_neg_paths\n",
    "val_paths = val_pos_paths+val_neg_paths\n",
    "test_paths = test_pos_paths+test_neg_paths\n",
    "\n",
    "random.Random(SEED).shuffle(train_paths)\n",
    "random.Random(SEED).shuffle(val_paths)\n",
    "random.Random(SEED).shuffle(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the training data: 20000\n",
      "length of the validation data: 5000\n",
      "length of the test data: 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"length of the training data: {}\".format(len(train_paths)))\n",
    "print (\"length of the validation data: {}\".format(len(val_paths)))\n",
    "print (\"length of the test data: {}\".format(len(test_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_collection(paths):\n",
    "    data = pd.DataFrame()\n",
    "    id_list = []\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    \n",
    "    for path in tqdm(paths):\n",
    "        file_name = path.split('/')[-1].split('.')[0]\n",
    "        id_list.append(file_name.split('_')[0])\n",
    "#         For 10 classes\n",
    "#         label_list.append(int(file_name.split('_')[1])-1)\n",
    "#         For 2 classes\n",
    "        label_list.append(int(file_name.split('_')[1])-1)\n",
    "        with open(path, 'r') as f:\n",
    "            text_list.append(f.read())\n",
    "    \n",
    "    data['ID'] = id_list\n",
    "    data['text'] = text_list\n",
    "    data['label'] = label_list    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:07<00:00, 2548.83it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 10148.93it/s]\n",
      "100%|██████████| 25000/25000 [00:01<00:00, 14526.28it/s]\n"
     ]
    }
   ],
   "source": [
    "training_dataset = dataset_collection(train_paths[:])\n",
    "val_dataset = dataset_collection(val_paths[:])\n",
    "test_dataset =dataset_collection(test_paths[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize_spacy(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if token.text not in punctuations]\n",
    "\n",
    "def tokenize_nltk(sent):\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    return [token.lower() for token in tokens if token not in punctuations]\n",
    "\n",
    "def tokenize_space(sent):\n",
    "    tokens = sent.split(' ')\n",
    "    return [token.lower() for token in tokens if token not in punctuations]\n",
    "\n",
    "def tokenize_dataset(dataset, mode='spacy'):\n",
    "    data_id = []\n",
    "    data_text = []\n",
    "    data_label = []\n",
    "    all_tokens = []\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        if mode == 'spacy':\n",
    "            tokens = tokenize_spacy(dataset.iloc[idx]['text'].replace('<br /><br />', ''))\n",
    "        elif mode == 'nltk':\n",
    "            tokens = tokenize_spacy(dataset.iloc[idx]['text'].replace('<br /><br />', ''))\n",
    "        elif mode == 'space':\n",
    "            tokens = tokenize_space(dataset.iloc[idx]['text'].replace('<br /><br />', ''))\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized mode!\")\n",
    "        data_id.append(dataset.iloc[idx]['ID'])\n",
    "        data_text.append(tokens)\n",
    "        data_label.append(dataset.iloc[idx]['label'])\n",
    "        all_tokens += tokens\n",
    "    return data_id, data_text, data_label, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, val_tokens_, val_label, _ = tokenize_dataset(val_dataset)\n",
    "_, train_tokens_, train_label, all_tokens_ = tokenize_dataset(training_dataset)\n",
    "_, test_tokens_, test_label, _ = tokenize_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(train_tokens, open('./train_tokens_spacy.p', 'wb'))\n",
    "# pickle.dump(train_label, open('./train_label_bi_spacy.p', 'wb'))\n",
    "# pickle.dump(all_tokens, open('./all_tokens_spacy.p', 'wb'))\n",
    "\n",
    "# pickle.dump(val_tokens, open('./val_tokens_spacy.p', 'wb'))\n",
    "# pickle.dump(val_label, open('./val_label_bi_spacy.p', 'wb'))\n",
    "\n",
    "# pickle.dump(test_tokens, open('./test_tokens_spacy.p', 'wb'))\n",
    "# pickle.dump(test_label, open('./test_label_bi_spacy.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens=pickle.load(open('./train_tokens.p', 'rb'))\n",
    "train_label=pickle.load(open('./train_label.p', 'rb'))\n",
    "all_tokens=pickle.load(open('./all_tokens.p', 'rb'))\n",
    "\n",
    "val_tokens = pickle.load(open('./val_tokens.p', 'rb'))\n",
    "val_label = pickle.load(open('./val_label.p', 'rb'))\n",
    "\n",
    "test_tokens = pickle.load(open('./test_tokens.p', 'rb'))\n",
    "test_label = pickle.load(open('./test_label.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:04<00:00, 4881.91it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4955.39it/s]\n",
      "100%|██████████| 25000/25000 [00:04<00:00, 5387.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def to_N_gram(dataset, N):\n",
    "    dataset_n_gram = []\n",
    "    all_tokens = []\n",
    "    for data in tqdm(dataset):\n",
    "        n_gram = []\n",
    "        for idx in range(len(data)-N+1):\n",
    "#             pdb.set_trace()\n",
    "            n_gram.append([' '.join(data[idx:idx+N])][0])\n",
    "        dataset_n_gram.append(n_gram)\n",
    "        all_tokens += n_gram\n",
    "    return dataset_n_gram, all_tokens\n",
    "\n",
    "train_tokens_bi, all_tokens_bi = to_N_gram(train_tokens, 2)\n",
    "val_tokens_bi, _ = to_N_gram(val_tokens, 2)\n",
    "test_tokens_bi, _ = to_N_gram(test_tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  1203340\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "VOCABULARY_SIZE = 40000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1 \n",
    "MAX_SENTENCE_LENGTH = 400\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    print (\"Total number of tokens: \", len(token_counter))\n",
    "    words, freq = zip(*token_counter.most_common(VOCABULARY_SIZE))\n",
    "    id2token = ['<pad>', '<unk>']+list(words)\n",
    "    token2id = dict(zip(id2token, range(2+len(words))))\n",
    "    return id2token, token2id\n",
    "\n",
    "id2token, token2id = build_vocab(all_tokens_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokens_id(idx):\n",
    "    print (\"Token {} corresponds to word {}\".format(idx, id2token[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "def token2id_dataset(text_dataset):\n",
    "    indices_data=[]\n",
    "    for text in text_dataset:\n",
    "        idx_list = [token2id[word] if word in token2id else UNK_IDX for word in text]\n",
    "#         idx_list = idx_list+[PAD_IDX]*(PAD_LEN-len(idx_list))\n",
    "        indices_data.append(idx_list)\n",
    "    return indices_data\n",
    "\n",
    "train_indices = token2id_dataset(train_tokens_bi)\n",
    "val_indices = token2id_dataset(val_tokens_bi)\n",
    "test_indices = token2id_dataset(test_tokens_bi)\n",
    "\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 400\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDB_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for datum in batch:\n",
    "#         pdb.set_trace()\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "#     for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDB_Dataset(train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDB_Dataset(val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDB_Dataset(test_indices, test_label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,10)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "emb_dim = 200\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/625], Validation Acc: 20.38\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 26.8\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 27.26\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 23.9\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 31.16\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 31.56\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 30.7\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 32.76\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 33.92\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.NLLLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "#     print (total)\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train(model, num_epochs, early_stopping_patience = 0):\n",
    "    prev_best = 0\n",
    "    count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "    #         pdb.set_trace()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "        val_acc = test_model(train_loader, model)\n",
    "        if val_acc >= prev_best:\n",
    "            prev_best = val_acc\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "        if count > early_stopping_patience:\n",
    "            print (\"Finished!\")\n",
    "            break\n",
    "    print ('Finished!')\n",
    "train(model, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_model(test_loader, model)\n",
    "print (test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
